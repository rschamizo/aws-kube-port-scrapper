= AWS Kube Port Scrapper

The following project aims to continuously obtain a report with the open ports of the nodes of an EKS cluster. The report can be exported to an s3 bucket, or to a DynamoDB table. An alert system is also carried out with prometheus and the Sns.

The complete and detailed documentation is built with link:https://antora-rschamizo.s3.eu-west-3.amazonaws.com/index.html[Antora Docs]. This README provides a quick overview of the project, and ways to partially implement the features. It also assumes certain knowledge about the topics covered. For the complete process visit link:https://antora-rschamizo.s3.eu-west-3.amazonaws.com/aws-kube-port-scrapper/0.1/full-execution.html[Scrapper Full Execution]

== Architecture

The architecture is based on a main element, the cronjob, which will create jobs in each of the nodes to obtain said information. That is, there are two separate applications, on the one hand the 'master-scrapper-scheduler' and on the other hand the 'node-port-scrapper'. The first one has the aws cli, as well as kubectl, and the credentials for both services. In this way it is able to create resources within the cluster.

The second is able to access the node and obtain the desired information. Once the 'node-port-scrapper' get the report from each node, the 'scheduler' gathers all the information, and uploads it to s3 and DynamoDB.

The project consists of the following modules:
* master-scrapper-scheduler -> Orchestrator application cronjob
* node-port-scrapper -> Application that extracts the report of each node
* terraform -> Terraform module to initialize the necessary resources in AWS
* ansible -> Orchestrator of all the flow, and of the creation of resources in kubernetes
* alerting -> Prometheus and Sns based alerting

== Quickstart

The fastest way to check the operation is to execute the following command:

[source,bash]
----
kubectl create -f samples/node-port-scrapper-sample
----

This executes the analysis, and returns the json with the required information in the logs. It uses a shell script, on top of an alpine image, and with the help of packages like 'jq', 'sed' builds the information as required.
For more information visit link:https://antora-rschamizo.s3.eu-west-3.amazonaws.com/aws-kube-port-scrapper/0.1/node-port-scrapper.html[Node Port Scrapper]

Se pueden excluir algunos puertos del reporte simplemente a√±adiendo dicho puerto a la variable de entorno **WHITE_LIST_PORT_STRING**, estando cada puerto separado por un espacio.

El comando que permite extraer la informacion de los puertos abiertos es el siguiente:

[source,bash]
....
netstat -tulpnl | grep LISTEN 
....

== Build Cronjob Application

The application needs credentials of an AWS user to work. Then it allows you to choose whether to upload the report to s3 and to a DynamoDB table. These features are optional, and can be disabled. In the case of being enabled, all the resources must be in the same region.

First of all we have to create the secret with the aws credentials file that will be mounted on the pods. The user must have permissions both on the aws resources and to interact with the kubernetes cluster. For example, in the simplest way (It is not recommended to use the personal user, it is better to create a user with the appropriate permissions. The terraform module creates it with the appropriate permissions)

[source,bash]
....
kubectl create secret generic aws-credentials --from-file=credentials=$HOME/.aws/credentials
....

Some ports can be excluded from the report by simply adding that port to the **WHITE_LIST_PORT_STRING** environment variable, each port being separated by a space.

[source,bash]
....
kubectl create -f samples/cronjob-sample.yaml
....

== Monitoring and Alerting

An alert system based on prometheus and AWS SNS have been created. First of all, the corresponding namespace must be created, for example 'monitoring'

[source,bash]
....
kubectl create ns monitoring
....

If we want to enable sns notifications, we must first create the alertmanager configuration file.

There is an example in samples/alertmanager.yaml, where we should insert the AWS user credentials, before creating the secret, and configure the Sns topic (which is assumed previously created manually or with the terraform module)

[source,bash]
....
kubectl create secret generic alertmanager-main -n monitoring --from-file=alertmanager.yaml=samples/alertmanager.yaml
....

The installation of Prometheus can be done through helm with the following command:

[source,bash]
....
helm install -n monitoring prometheus-k8s prometheus-community/prometheus --version 15.1.3 --values samples/prometheus-helm-values.yaml
....

The personalized alerts of the project are already included here. They are most easily found in samples/alertmanager-scrapperRules.yaml. These are in CRD format that is used by the prometheus-operator. You can choose to install the version with the operator with the following command (if not installed with helm)

[source,bash]
....
kubectl create -f samples/prometheus-operator-mode
....

5 alerts have been defined, which can be seen in the alerts tab if you access prometheus

[source,bash]
....
kubectl port-forward -n monitoring svc/prometheus-k8s-server 9090:80
....
